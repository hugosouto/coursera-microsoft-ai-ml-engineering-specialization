{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction\n",
    "\n",
    "Web scraping is a powerful method for acquiring data from websites, especially when the information you need isn’t readily available in a structured format. By setting up a web scraper in your local environment, you can automate the process of gathering large amounts of data from the web. \n",
    "\n",
    "By the end of this reading, you will be able to: \n",
    "\n",
    "Set up a basic data scraper using Python, including code snippets and explanations to help you get started."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Before diving into the code, ensure you have the following tools installed on your local environment:\n",
    "\n",
    "- Python 3.x: Python is the language we’ll use to build our web scraper.\n",
    "\n",
    "- pip: pip is Python’s package installer, which you’ll use to install the necessary libraries.\n",
    "\n",
    "- A code editor: Examples include Jupyter Notebooks, VS Code, PyCharm, or even a simple text editor such as Sublime Text.\n",
    "\n",
    "You’ll also need a basic understanding of HTML, as web scraping involves interacting with the HTML structure of a web page."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 1: Microsoft Learn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Import the necessary libraries\n",
    "\n",
    "Start by importing the libraries you’ll need, if they’re not already in your kernel:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "import time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2: Send an HTTP request to the website\n",
    "\n",
    "Use the requests library to send an HTTP GET request to the website you want to scrape:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Request successful!\n"
     ]
    }
   ],
   "source": [
    "url = 'https://learn.microsoft.com/en-us/python/api/azure-ai-ml/azure.ai.ml.automl?view=azure-python'\n",
    "\n",
    "try:\n",
    "    response = requests.get(url)\n",
    "    response.raise_for_status()  # Raises an HTTPError for bad responses\n",
    "except requests.exceptions.HTTPError as err:\n",
    "    print('HTTP error occurred:', err)\n",
    "except Exception as err:\n",
    "    print('Other error occurred:', err)\n",
    "\n",
    "# Check if the request was successful\n",
    "if response.status_code == 200:\n",
    "    print('Request successful!')\n",
    "else:\n",
    "    print('Failed to retrieve the webpage')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Parse the HTML content\n",
    "\n",
    "Once you’ve successfully retrieved the web page, use BeautifulSoup to parse the HTML content:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "azure.ai.ml.automl package | Microsoft Learn\n"
     ]
    }
   ],
   "source": [
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Print the title of the webpage to verify\n",
    "print(soup.title.text)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Extract the data you need\n",
    "\n",
    "Now that you have the HTML parsed, you can start extracting the data you’re interested in. Let’s say you want to scrape a list of items from a table on the web page:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "Name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Description",
         "rawType": "object",
         "type": "string"
        }
       ],
       "conversionMethod": "pd.DataFrame",
       "ref": "124fb5c1-7100-4674-b4a7-8c94d9c60d76",
       "rows": [
        [
         "0",
         "training_data",
         "Input   The training data to be used within the experiment. It should contain both training features and a label column (optionally a sample weights column)."
        ],
        [
         "1",
         "target_column_name",
         "str   The name of the label column. This parameter is applicable to training_data, validation_data and test_data parameters"
        ],
        [
         "2",
         "primary_metric",
         "The metric that Automated Machine Learning will optimize for model selection. Automated Machine Learning collects more metrics than it can optimize. For more information on how metrics are calculated, see https://learn.microsoft.com/azure/machine-learning/how-to-configure-auto-train#primary-metric. Acceptable values: accuracy, AUC_weighted, norm_macro_recall, average_precision_score_weighted, and precision_score_weighted Defaults to accuracy"
        ],
        [
         "3",
         "enable_model_explainability",
         "bool   Whether to enable explaining the best AutoML model at the end of all AutoML training iterations. The default is None. For more information, see Interpretability: model explanations in automated machine learning."
        ],
        [
         "4",
         "weight_column_name",
         "str   The name of the sample weight column. Automated ML supports a weighted column as an input, causing rows in the data to be weighted up or down. If the input data is from a pandas.DataFrame which doesn't have column names, column indices can be used instead, expressed as integers. This parameter is applicable to training_data and validation_data parameters"
        ],
        [
         "5",
         "validation_data",
         "Input   The validation data to be used within the experiment. It should contain both training features and label column (optionally a sample weights column). Defaults to None"
        ],
        [
         "6",
         "validation_data_size",
         "float   What fraction of the data to hold out for validation when user validation data is not specified. This should be between 0.0 and 1.0 non-inclusive. Specify validation_data to provide validation data, otherwise set n_cross_validations or validation_data_size to extract validation data out of the specified training data. For custom cross validation fold, use cv_split_column_names. For more information, see Configure data splits and cross-validation in automated machine learning. Defaults to None"
        ],
        [
         "7",
         "n_cross_validations",
         "Union[str, int] \t\t  How many cross validations to perform when user validation data is not specified. Specify validation_data to provide validation data, otherwise set n_cross_validations or validation_data_size to extract validation data out of the specified training data. For custom cross validation fold, use cv_split_column_names. For more information, see Configure data splits and cross-validation in automated machine learning. Defaults to None"
        ],
        [
         "8",
         "cv_split_column_names",
         "List[str] \t\t  List of names of the columns that contain custom cross validation split. Each of the CV split columns represents one CV split where each row are either marked 1 for training or 0 for validation. Defaults to None"
        ],
        [
         "9",
         "test_data",
         "Input   The Model Test feature using test datasets or test data splits is a feature in Preview state and might change at any time. The test data to be used for a test run that will automatically be started after model training is complete. The test run will get predictions using the best model and will compute metrics given these predictions. If this parameter or the test_data_size parameter are not specified then no test run will be executed automatically after model training is completed. Test data should contain both features and label column. If test_data is specified then the target_column_name parameter must be specified. Defaults to None"
        ],
        [
         "10",
         "test_data_size",
         "float   The Model Test feature using test datasets or test data splits is a feature in Preview state and might change at any time. What fraction of the training data to hold out for test data for a test run that will automatically be started after model training is complete. The test run will get predictions using the best model and will compute metrics given these predictions. This should be between 0.0 and 1.0 non-inclusive. If test_data_size is specified at the same time as validation_data_size, then the test data is split from training_data before the validation data is split. For example, if validation_data_size=0.1, test_data_size=0.1 and the original training data has 1000 rows, then the test data will have 100 rows, the validation data will contain 90 rows and the training data will have 810 rows. For regression based tasks, random sampling is used. For classification tasks, stratified sampling is used. Forecasting does not currently support specifying a test dataset using a train/test split. If this parameter or the test_data parameter are not specified then no test run will be executed automatically after model training is completed. Defaults to None"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 11
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Name</th>\n",
       "      <th>Description</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>training_data</td>\n",
       "      <td>Input   The training data to be used within th...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>target_column_name</td>\n",
       "      <td>str   The name of the label column. This param...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>primary_metric</td>\n",
       "      <td>The metric that Automated Machine Learning wil...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>enable_model_explainability</td>\n",
       "      <td>bool   Whether to enable explaining the best A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>weight_column_name</td>\n",
       "      <td>str   The name of the sample weight column. Au...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>validation_data</td>\n",
       "      <td>Input   The validation data to be used within ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>validation_data_size</td>\n",
       "      <td>float   What fraction of the data to hold out ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>n_cross_validations</td>\n",
       "      <td>Union[str, int] \\t\\t  How many cross validatio...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>cv_split_column_names</td>\n",
       "      <td>List[str] \\t\\t  List of names of the columns t...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>test_data</td>\n",
       "      <td>Input   The Model Test feature using test data...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>test_data_size</td>\n",
       "      <td>float   The Model Test feature using test data...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Name  \\\n",
       "0                 training_data   \n",
       "1            target_column_name   \n",
       "2                primary_metric   \n",
       "3   enable_model_explainability   \n",
       "4            weight_column_name   \n",
       "5               validation_data   \n",
       "6          validation_data_size   \n",
       "7           n_cross_validations   \n",
       "8         cv_split_column_names   \n",
       "9                     test_data   \n",
       "10               test_data_size   \n",
       "\n",
       "                                          Description  \n",
       "0   Input   The training data to be used within th...  \n",
       "1   str   The name of the label column. This param...  \n",
       "2   The metric that Automated Machine Learning wil...  \n",
       "3   bool   Whether to enable explaining the best A...  \n",
       "4   str   The name of the sample weight column. Au...  \n",
       "5   Input   The validation data to be used within ...  \n",
       "6   float   What fraction of the data to hold out ...  \n",
       "7   Union[str, int] \\t\\t  How many cross validatio...  \n",
       "8   List[str] \\t\\t  List of names of the columns t...  \n",
       "9   Input   The Model Test feature using test data...  \n",
       "10  float   The Model Test feature using test data...  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Find the table containing the data\n",
    "table = soup.find('table', {'title': 'python-keyword-only-parameter-table'})  # Replace 'data-table' with the actual id or class of the table\n",
    "\n",
    "# Extract table rows\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "# Loop through the rows and extract data\n",
    "data = []\n",
    "for row in rows[1:]: # Skipping the header row\n",
    "    cols = row.find_all('td')\n",
    "    if len(cols) == 2:\n",
    "        cols = [col.text.strip() for col in cols]\n",
    "        data.append(cols)\n",
    "        time.sleep(0.5)  # Adds a delay before the next request\n",
    "\n",
    "# Convert the data into a pandas DataFrame for easier manipulation\n",
    "# Replace line breaks in the 'Description' column with spaces\n",
    "df = pd.DataFrame(data, columns=['Name', 'Description']) \n",
    "df['Description'] = df['Description'].str.replace('\\n', ' ', regex=True)\n",
    "\n",
    "# Remove any rows with missing values\n",
    "df = df.dropna()\n",
    "\n",
    "# Display the scraped data\n",
    "df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 5: Save the scraped data\n",
    "\n",
    "Finally, you can save the scraped data to a file for further analysis:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('scraped_data_microsoft_learn.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example 2: Scraping information from Wikipedia"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Title: Cloud-computing comparison - Wikipedia\n",
      "                      Provider Launched Block storage Assignable IPs  \\\n",
      "0        Google Cloud Platform     2013           Yes             No   \n",
      "1  Oracle Cloud Infrastructure     2014           Yes            Yes   \n",
      "2          Amazon Web Services     2006           Yes            Yes   \n",
      "3                    IBM Cloud     2005           Yes            Yes   \n",
      "4              Microsoft Azure     2010           Yes            Yes   \n",
      "\n",
      "  SMTP support IOPS Guaranteed minimum Security  \\\n",
      "0        No[1]                     Yes   Yes[2]   \n",
      "1          Yes                     Yes   Yes[5]   \n",
      "2   Partial[6]                     Yes   Yes[7]   \n",
      "3        No[9]                     Yes  Yes[10]   \n",
      "4      Yes[11]                     Yes  Yes[12]   \n",
      "\n",
      "                                           Locations             Notes  \n",
      "0  br, ca, cl, us, be, ch, de, es, fi, it, po, nl...  SMTP blocked.[4]  \n",
      "1  us, ca, br, de, uk, nl, ch, in, aus, jp, kr, saud                    \n",
      "2  us, ca, br, ie, de, uk, cn, sg, au, jp, kr, in...   List of bugs[8]  \n",
      "3  us, gb, fr, de, nl, in, au, hk, kr, it, jp, no...                    \n",
      "4  ca, us, br, ie, nl, de, uk, cn, au, jp, in, kr...  List of bugs[13]  \n"
     ]
    }
   ],
   "source": [
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "import pandas as pd\n",
    "\n",
    "# Send an HTTP request to the webpage\n",
    "url = 'https://en.wikipedia.org/wiki/Cloud-computing_comparison'  \n",
    "response = requests.get(url)\n",
    "\n",
    "# Parse the HTML content\n",
    "soup = BeautifulSoup(response.content, 'html.parser')\n",
    "\n",
    "# Print the title of the webpage to verify\n",
    "print(\"Title: \" + soup.title.text)\n",
    "\n",
    "# Find the table containing the data (selecting the first table by default)\n",
    "table = soup.find('table')\n",
    "\n",
    "# Extract table rows\n",
    "rows = table.find_all('tr')\n",
    "\n",
    "# Extract headers from the first row (using <th> tags)\n",
    "headers = [header.text.strip() for header in rows[0].find_all('th')]\n",
    "\n",
    "# Loop through the rows and extract data (skip the first row with headers)\n",
    "data = []\n",
    "for row in rows[1:]:  # Start from the second row onwards\n",
    "    cols = row.find_all('td')\n",
    "    cols = [col.text.strip() for col in cols]\n",
    "    data.append(cols)\n",
    "\n",
    "# Convert the data into a pandas DataFrame, using the extracted headers as column names\n",
    "df = pd.DataFrame(data, columns=headers)\n",
    "\n",
    "# Display the first few rows of the DataFrame to verify\n",
    "print(df.head())  \n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv('scraped_data_wikipedia.csv', index=False)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
